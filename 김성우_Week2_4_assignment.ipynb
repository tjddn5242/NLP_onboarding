{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week2_4 Assignment\n","\n","## [BASIC](#Basic) \n","- 커스텀 모듈(`helper.py`)에서 **클래스와 함수를 임포트**할 수 있다.\n","- **autograd**의 개념 복습\n","\n","\n","## [CHALLENGE](#Challenge)\n","- train() 함수에 **epoch, scheduler, grad_clipping**을 추가할 수 있다.\n","- **validate() 함수를 구현**할 수 있다.\n","\n","\n","## [ADVANCED](#Advanced)\n","- train() 함수를 사용해 데이터를 **4 epoch 학습**할 수 있다. \n","- **predict 함수를 구현**할 수 있다. \n","- **evaluation metric 구현**할 수 있다. \n","    - accuracy\n","\n","\n","\n","### Reference\n","- [Pytorch Autograd Explain official document](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:47.370876Z","start_time":"2022-02-02T04:01:46.520392Z"},"id":"KSX-wQA1RD1h","executionInfo":{"status":"ok","timestamp":1646292330526,"user_tz":-540,"elapsed":1990,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:47.375658Z","start_time":"2022-02-02T04:01:47.372242Z"},"id":"MH7RJjtZXOHf","executionInfo":{"status":"ok","timestamp":1646292330526,"user_tz":-540,"elapsed":6,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# set seed\n","seed = 7777\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:07:00.849353Z","start_time":"2022-01-31T13:06:56.187962Z"},"id":"62plMahMWr0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292337734,"user_tz":-540,"elapsed":7213,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"18390133-7ec7-4f18-907f-76a599377a63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6WagJcj-Ud4L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292342026,"user_tz":-540,"elapsed":4306,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"c5367568-499b-40fe-9489-9b98797e42c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd './drive/MyDrive/Classroom/AI심화과정'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NfWtTlNbvrf1","executionInfo":{"status":"ok","timestamp":1646292342026,"user_tz":-540,"elapsed":6,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"864bba60-236b-4fe1-f842-c0a1eb384064"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Classroom/AI심화과정\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"qnETqIqdVApF","executionInfo":{"status":"ok","timestamp":1646292342027,"user_tz":-540,"elapsed":5,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# 어제 자신이 구현한 helper.py 모듈 경로를 입력\n","sys.path.append('./helper.py')"]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:49.735578Z","start_time":"2022-02-02T04:01:49.475969Z"},"id":"N84mZeYMUFxJ","executionInfo":{"status":"ok","timestamp":1646292344351,"user_tz":-540,"elapsed":2329,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# helper 모듈을 import하면 이전에 구현했던 다양한 함수 및 클래스를 사용할 수 있음 \n","# 함수: set_device()\n","# 함수: custom_collate_fn() \n","# 클래스: CustomDataset\n","# 클래스: CustomClassifier\n","# 가 import 됨\n","\n","from helper import *\n","from torch.utils.data import RandomSampler, SequentialSampler, DataLoader, random_split"]},{"cell_type":"code","execution_count":8,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:49.771743Z","start_time":"2022-02-02T04:01:49.736866Z"},"id":"oR5EWmh5UFxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292344353,"user_tz":-540,"elapsed":35,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"1cd30c3f-5f37-4b67-dd51-c0c56c7e6f8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["# available GPUs : 1\n","GPU name : Tesla T4\n","device: cuda\n"]}],"source":["# device\n","device = set_device()\n","print(f\"device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"pkNxrCV45Q3m"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"9YBUQykS5Q3n"},"source":["### 모듈에서 클래스와 함수를 임포트해 다음을 구현\n","- train_dataset, train_dataloader\n","- valid_dataset, valid_dataloader\n","- test_dataset, test_dataloader"]},{"cell_type":"code","source":["# train dataframe 다운로드\n","# !wget https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/e56006adfac42f8a2975db0ebbe60eacbe1c6b11/data/sample_df.csv"],"metadata":{"id":"cjNksUEwGACb","executionInfo":{"status":"ok","timestamp":1646292344354,"user_tz":-540,"elapsed":32,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# test dataframe 다운로드\n","# !wget https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/main/data/sample_df_test.csv"],"metadata":{"id":"kXfk8ZEHGB0v","executionInfo":{"status":"ok","timestamp":1646292344355,"user_tz":-540,"elapsed":32,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.037044Z","start_time":"2022-02-02T04:01:52.707669Z"},"id":"KVo5dPnmUFxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292344355,"user_tz":-540,"elapsed":32,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"4d8c7c94-f584-4e00-b3fb-ea3d52a64e49"},"outputs":[{"output_type":"stream","name":"stdout","text":["train shape : (10000, 3)\n","test shape : (1000, 3)\n"]}],"source":["# 학습 & 평가 데이터셋 로드\n","# 학습 및 평가 샘플 데이터 개수는 각각 10,000개, 1,000개\n","\n","df_train = pd.read_csv('sample_df.csv')\n","df_test = pd.read_csv('sample_df_test.csv')\n","\n","print(f\"train shape : {df_train.shape}\")\n","print(f\"test shape : {df_test.shape}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.085720Z","start_time":"2022-02-02T04:01:53.081413Z"},"id":"Ql82Ew2VUFxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292344356,"user_tz":-540,"elapsed":27,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"f5b73fd0-4161-4306-e9bc-2a855b2c916c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Dataset len: 10000\n","Train Dataset 1st element: ('나 이거 더빙을 누가하는지 모르고 봤는데 왠지 더빙이 구리더라...더빙이 너무 별로였음.', 0)\n","Test Dataset len: 1000\n","Test Dataset 1st element: ('신용문객잔 보고 후속편인줄 알고 봤더만 완전 개판이네 18.. 이련결 그냥 절에나 쳐 들어 가라.. 회오리에서 싸우는 신 참 가관이더라 .. 서극도 완전 쓰레기 감독이 다 됐구나.. 액션도 쓰레기고 배우들 연기도 참 가관이더라 18', 0)\n"]}],"source":["# Dataset 구현\n","# helper.py에 있는 CustomDataset 활용하여 train datset, test dataset 만들기\n","\n","train_dataset = CustomDataset(list(df_train.document.values), list(df_train.label.values))\n","test_dataset = CustomDataset(list(df_test.document.values), list(df_test.label.values))\n","\n","print(f\"Train Dataset len: {len(train_dataset)}\")\n","print(f\"Train Dataset 1st element: {train_dataset[0]}\")\n","\n","print(f\"Test Dataset len: {len(test_dataset)}\")\n","print(f\"Test Dataset 1st element: {test_dataset[0]}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.152070Z","start_time":"2022-02-02T04:01:53.145410Z"},"id":"7WUY6h8WUFxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292344356,"user_tz":-540,"elapsed":21,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"fcdd4730-d2d0-4e95-ad1c-f3db05fcae5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset len: 9000\n","Valid dataset len: 1000\n"]}],"source":["# Train Dataset을 학습과 검증 셋으로 분리\n","# 학습 셋과 검증 셋의 비율은 9:1\n","# torch.utils.data에서 제공되는 데이터 세트를 임의로 분할할 수 있는 함수 찾아서 사용\n","n_train_sample = df_train.shape[0]\n","\n","n_train = int(n_train_sample*0.9)\n","n_valid = n_train_sample - n_train \n","train_dataset, valid_dataset = random_split(train_dataset, [n_train, n_valid])\n","\n","print(f\"Train dataset len: {len(train_dataset)}\")\n","print(f\"Valid dataset len: {len(valid_dataset)}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.268838Z","start_time":"2022-02-02T04:01:53.263780Z"},"id":"H5nc7SpTUFxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292344356,"user_tz":-540,"elapsed":16,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"3e3dcd4e-77aa-4adb-c512-f5ccdb5430d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataloader # steps: 282\n","Valid dataloader # steps: 16\n","Test dataloader # steps: 16\n"]}],"source":["# DataLoader 구현\n","# train과 validation의 batch size는 각각 32, 64로 설정\n","# test의 batch size는 validation과 동일\n","# train에 사용할 DataLoader에서는 sampler로 RandomSampler 사용\n","# validation과 test에 사용할 DataLoader에서는 sampler로 SequentialSampler 사용\n","# 모든 DataLoader의 collate_fn은 helper.py에 있는 custom_collate_fn 사용\n","\n","train_batch_size = 32\n","valid_batch_size = 64\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, collate_fn=custom_collate_fn, sampler=RandomSampler(train_dataset))\n","\n","valid_dataloader = DataLoader(valid_dataset, batch_size=valid_batch_size, collate_fn=custom_collate_fn, sampler=SequentialSampler(valid_dataset))\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=valid_batch_size, collate_fn=custom_collate_fn, sampler=SequentialSampler(test_dataset))\n","\n","print(f\"Train dataloader # steps: {len(train_dataloader)}\")\n","print(f\"Valid dataloader # steps: {len(valid_dataloader)}\")\n","print(f\"Test dataloader # steps: {len(test_dataloader)}\")"]},{"cell_type":"markdown","metadata":{"id":"9kEgqvBIUFxN"},"source":["### `auto_grad` 개념 복습\n","- torch의 `auto_grad` 기능\n","    - pytorch는 `requires_grad` 파리미터의 값이 True인 텐서에 한해서 미분값을 자동으로 계산한다.\n","    - 미분값은 `loss.backward()` 가 호출될 때 자동으로 계산된다."]},{"cell_type":"code","execution_count":15,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:45:23.502936Z","start_time":"2022-01-31T13:45:20.029987Z"},"id":"oYjYpQ1DUFxN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292346723,"user_tz":-540,"elapsed":2379,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"3bcc5669-a9aa-4483-c385-b68529d5b30a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# helper.py에 있는 CustomClassifier 모델을 로드해 model_freeze 변수에 instance를 생성\n","# hidden_size=768\n","# n_label=2\n","# freeze_base=True\n","\n","model_freeze = CustomClassifier(hidden_size=768, n_label=2)"]},{"cell_type":"code","execution_count":16,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:45:34.604914Z","start_time":"2022-01-31T13:45:34.586711Z"},"id":"XxNFh8KZUFxN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292346723,"user_tz":-540,"elapsed":28,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"d14cfc1e-a945-4bdf-d9e4-8c467d231707"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.parameters of CustomClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=768, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=32, out_features=2, bias=True)\n","  )\n",")>"]},"metadata":{},"execution_count":16}],"source":["# model_freeze 모델의 모든 파라미터를 출력해보고 아래 질문에 답해 보자\n","\n","model_freeze.parameters"]},{"cell_type":"code","source":["model_freeze.bert.encoder.layer[0].attention.self.query.weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBfVyQ_-4NDW","executionInfo":{"status":"ok","timestamp":1646292346724,"user_tz":-540,"elapsed":27,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"a82c8957-ffcc-471b-d75e-c70c7e43035d"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0136,  0.0162, -0.0616,  ..., -0.0227, -0.0551, -0.0385],\n","        [ 0.0549, -0.0462, -0.0229,  ...,  0.0028, -0.0088,  0.0135],\n","        [ 0.0275,  0.0846,  0.0093,  ..., -0.0018,  0.0184, -0.0297],\n","        ...,\n","        [ 0.0105, -0.0079, -0.0426,  ...,  0.0378, -0.0219,  0.0065],\n","        [-0.0283,  0.0635,  0.0240,  ...,  0.0183,  0.0244, -0.0108],\n","        [ 0.0601, -0.0081,  0.0419,  ...,  0.0085,  0.0259,  0.0199]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["print(model_freeze.classifier[0].weight.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cc60yfqO48Xj","executionInfo":{"status":"ok","timestamp":1646292346725,"user_tz":-540,"elapsed":18,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"f69a57a8-cc69-45f3-a9e2-c93c4c5f8cd1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"markdown","metadata":{"id":"KloNNAKI5Q3r"},"source":["### `auto_grad` 개념 및 모델 구조 복습을 위해 다음 항목에 답해 보자\n","- `bert.encoder.layer.0.attention.self.query.weight` 텐서의 gradient는 True인 상태인가?\n","> 네 ```requires_grad=True```입니다.\n","- `classifier.0.weight` 텐서의 shape은? \n","> torch.Size([32, 768]) 입니다.\n","- `classifier.0.weight` 텐서는 freeze 상태인가 ? \n","> 아닙니다. ```requires_grad=True```이므로 unfreeze 상태입니다.\n","- `classifier.0.weight` 텐서의 gradient 값은 무엇인가? \n","> None입니다."]},{"cell_type":"markdown","metadata":{"id":"4iIrHg1xUFxP"},"source":["### 위 모델 (`model_freeze`)의 모든 파라미터의 gradient를 freeze 해보자"]},{"cell_type":"code","execution_count":19,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:49:26.820569Z","start_time":"2022-01-31T13:49:26.816511Z"},"id":"sHkaFgC8UFxP","executionInfo":{"status":"ok","timestamp":1646292346725,"user_tz":-540,"elapsed":13,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# 모든 파라미터의 gradient를 freeze 해보고 제대로 변경되었는지 \b확인하기 위해 모델의 모든 파라미터를 출력해보자.\n","\n","for param in model_freeze.parameters():\n","    param.requires_grad=False"]},{"cell_type":"code","source":["model_freeze.parameters"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFS10DDk9XlL","executionInfo":{"status":"ok","timestamp":1646292346726,"user_tz":-540,"elapsed":14,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"1294e94a-80d5-4a5f-e94b-58a74daeb7c6"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.parameters of CustomClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=768, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=32, out_features=2, bias=True)\n","  )\n",")>"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"NsMgM3sK5Q3t"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"vUn-6PFP5Q3t"},"source":["### `scheduler` 를 생성 \n","- 스케쥴러를 알기 전에 먼저 `epoch`의 개념을 알아야 한다. Epoch는 dataset를 **몇 번 반복**해 학습할 것인지를 의미한다. 만약 dataset의 개수가 2,000개이고 epoch을 2번 학습하게 되면 총 4,000개의 데이터를 학습하게 된다.   \n","- 스케쥴러는 epoch에 따라 learning rate의 값을 조정하는 것을 의미한다. \n","- 예를 들어 [여기](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)의 그림에서 볼 수 있듯이 `get_linear_schedule_with_warmup`는 특정 step까지는 learning rate를 천천히 상승시키다가 고점에 도달하면 다시 하락시킨다. "]},{"cell_type":"markdown","metadata":{"id":"_FuADvuT5Q3t"},"source":["### `model`, `optimizer`, `scheduler`를 초기화(=인스턴스 생성)하는 함수를 구현하라"]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:59.217735Z","start_time":"2022-02-02T04:01:59.210482Z"},"id":"-sE7xjYcRD1p","executionInfo":{"status":"ok","timestamp":1646292350153,"user_tz":-540,"elapsed":3436,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from torch.optim import AdamW\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import get_linear_schedule_with_warmup, get_constant_schedule"]},{"cell_type":"code","execution_count":22,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:59.549660Z","start_time":"2022-02-02T04:01:59.545752Z"},"id":"2eTFXzy8VK9R","executionInfo":{"status":"ok","timestamp":1646292350153,"user_tz":-540,"elapsed":6,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# model:CustomClassifier 사용, hidden size는 768, label 개수는 2\n","# optimizer: AdamW 사용, learning rate는 2e-5\n","# scheduler: transformers.get_linear_schedule_with_warmup 함수 사용, 단, num_warmup_steps 매개 변수는 사용하지 않음\n","\n","def initializer(train_dataloader, epochs=2):\n","    \"\"\"\n","    모델, 옵티마이저, 스케쥴러를 초기화한 후 반환\n","    \"\"\"\n","    \n","    model = CustomClassifier(hidden_size=768, n_label=2)\n","\n","    optimizer = AdamW(model.parameters(),lr=2e-5)\n","\n","    total_steps = len(train_dataloader) * epochs\n","    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","    return model, optimizer, scheduler"]},{"cell_type":"markdown","metadata":{"id":"Xz-8_5as5Q3u"},"source":["### model, optimizer, scheduler의 파라미터 저장하는 함수를 구현하라"]},{"cell_type":"code","execution_count":23,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:02:02.786877Z","start_time":"2022-02-02T04:02:02.783726Z"},"id":"vIP1BjFA5Q3u","executionInfo":{"status":"ok","timestamp":1646292350153,"user_tz":-540,"elapsed":5,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# 모델 저장 함수 구현\n","\n","def save_checkpoint(path, model, optimizer, scheduler, epoch, loss):\n","    file_name = f'{path}/model.ckpt.{epoch}'\n","    \n","    # torch.save 함수 참고\n","    torch.save(\n","        {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'loss' : loss\n","        }, \n","        file_name\n","    )\n","    \n","    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"]},{"cell_type":"markdown","metadata":{"id":"a3BUrgtJ5Q3v"},"source":["### `validate()` 함수 구현 \n","- `validate()` 함수 내 model의 상태는 **evaluate**이어야 한다. evaluate 상태의 model은 dropout을 진행하지 않는다. \n","- **forward**를 진행할 때 `with torch.no_grad(): ...` 설정해 미분 계산을 방지한다.\n"]},{"cell_type":"code","execution_count":24,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:02:11.636684Z","start_time":"2022-02-02T04:02:11.631550Z"},"id":"VHpuV0CXUFxR","executionInfo":{"status":"ok","timestamp":1646292350154,"user_tz":-540,"elapsed":5,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# input: model, valid_dataloader\n","# output: loss, 정확도\n","\n","def validate(model, valid_dataloader):\n","  global loss_fct\n","   \n","  # 모델을 evaluate 모드로 설정 & device 할당\n","  model.eval()\n","  model.to(device)\n","    \n","  total_loss, total_acc= 0,0\n","        \n","  for step, batch in enumerate(valid_dataloader):\n","        \n","    # tensor 연산 전, 각 tensor에 device 할당\n","    batch = tuple(item.to(device) for item in batch)\n","            \n","    batch_input, batch_label = batch\n","            \n","    # gradient 계산하지 않고 forward 진행\n","    with torch.no_grad():\n","        logits = model(**batch_input)\n","            \n","    # loss\n","    batch_label = batch_label.to(torch.long)\n","    loss = loss_fct(logits, batch_label)\n","    total_loss += loss.item()\n","        \n","    # accuracy\n","    probs = F.softmax(logits, dim=1)\n","    preds = torch.argmax(probs, dim=1).flatten()\n","    acc = (preds == batch_label).cpu().numpy().mean()\n","    total_acc+=acc\n","    \n","    total_loss = total_loss/(step+1)\n","    total_acc = total_acc/(step+1)*100\n","\n","    return total_loss, total_acc\n"]},{"cell_type":"markdown","metadata":{"id":"NukaJc15UFxQ"},"source":["### `train()` 함수에 `epoch`와 `clip_grad_norm` 추가\n","- data_loader를 `epoch`만큼 반복하면서 학습하도록 `train()` 함수를 수정하라\n","- `gradient cliping`은 미분 값 너무 큰 경우 gradient exploding되는 현상을 막기 위해 미분값이 `threshold`를 넘을 경우 특정 비율을 미분 값에 곱해 크기를 줄여준다.\n","- Reference\n","  - [clip_grad_norm_ official document](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n","  - [그래디언트 클립핑 설명 한국어 블로그](https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-6/05-gradient-clipping)"]},{"cell_type":"code","execution_count":25,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:02:10.624280Z","start_time":"2022-02-02T04:02:10.615781Z"},"id":"ZvY5rxDKHQAp","executionInfo":{"status":"ok","timestamp":1646292350154,"user_tz":-540,"elapsed":5,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# 위에서 구현한 모델 저장 함수(save_checkpoint)와 validate 함수도 추가해보자\n","\n","loss_fct = CrossEntropyLoss()\n","\n","def train(model, train_dataloader, valid_dataloader=None, epochs=2):\n","        global scheduler, loss_fct\n","        \n","        # train_dataloaer 학습을 epochs만큼 반복\n","        for epoch in range(epochs):\n","            print(f\"*****Epoch {epoch} Train Start*****\")\n","            \n","            # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n","            total_loss, batch_loss, batch_count = 0,0,0\n","        \n","            # model을 train 모드로 설정 & device 할당\n","            model.train()\n","            model.to(device)\n","            \n","            # data iterator를 돌면서 하나씩 학습\n","            for step, batch in enumerate(train_dataloader):\n","                batch_count+=1\n","                \n","                # tensor 연산 전, 각 tensor에 device 할당\n","                batch = tuple(item.to(device) for item in batch)\n","            \n","                batch_input, batch_label = batch\n","            \n","                # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n","                model.zero_grad()\n","            \n","                # forward\n","                logits = model(**batch_input)\n","            \n","                # loss\n","                batch_label = batch_label.to(torch.long)\n","                loss = loss_fct(logits, batch_label)\n","                batch_loss += loss.item()\n","                total_loss += loss.item()\n","            \n","                # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n","                loss.backward()\n","                \n","                # gradient clipping 적용 (max_norm = 1)\n","                clip_grad_norm_(model.parameters(), max_norm=1)\n","                \n","                # optimizer & scheduler 업데이트\n","                optimizer.step()\n","                scheduler.step()\n","                \n","                # 배치 10개씩 처리할 때마다 평균 loss와 lr를 출력\n","                if (step % 10 == 0 and step != 0):\n","                    learning_rate = optimizer.param_groups[0]['lr']\n","                    print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate}, Avg Loss : {batch_loss / batch_count:.4f}\")\n","\n","                    # reset \n","                    batch_loss, batch_count = 0,0\n","\n","            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n","            print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n","            \n","            if valid_dataloader is not None:\n","                print(f\"*****Epoch {epoch} Valid Start*****\")\n","                valid_loss, valid_acc = validate(model, valid_dataloader)\n","                print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid Acc : {valid_acc:.2f}\")\n","                print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n","            \n","            # checkpoint 저장\n","            save_checkpoint('.', model, optimizer, scheduler, epoch, loss)\n","                \n","        print(\"Train Completed. End Program.\")"]},{"cell_type":"markdown","metadata":{"id":"4NWKzxIaf1QJ"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"gFWnii7a5Q3w"},"source":["### 학습 데이터를 epoch 4까지 학습\n","- 매 epoch마다 다음을 수행한다.\n","  - 학습이 끝난 후 validate() 함수 실행 \n","  - validate() 함수가 끝난 후 model save 함수 실행"]},{"cell_type":"code","execution_count":26,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:02:11.377612Z","start_time":"2022-02-02T04:02:20.931961Z"},"id":"7Er1qKtsf1QJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292844952,"user_tz":-540,"elapsed":494803,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"58287548-5822-4573-e58e-7133eac0998d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Total train steps with 4 epochs: 1128\n","*****Epoch 0 Train Start*****\n","Epoch: 0, Step : 10, LR : 1.9804964539007094e-05, Avg Loss : 0.6778\n","Epoch: 0, Step : 20, LR : 1.962765957446809e-05, Avg Loss : 0.5932\n","Epoch: 0, Step : 30, LR : 1.945035460992908e-05, Avg Loss : 0.5555\n","Epoch: 0, Step : 40, LR : 1.927304964539007e-05, Avg Loss : 0.5090\n","Epoch: 0, Step : 50, LR : 1.9095744680851064e-05, Avg Loss : 0.4311\n","Epoch: 0, Step : 60, LR : 1.891843971631206e-05, Avg Loss : 0.4442\n","Epoch: 0, Step : 70, LR : 1.8741134751773053e-05, Avg Loss : 0.4497\n","Epoch: 0, Step : 80, LR : 1.8563829787234043e-05, Avg Loss : 0.4112\n","Epoch: 0, Step : 90, LR : 1.8386524822695038e-05, Avg Loss : 0.3910\n","Epoch: 0, Step : 100, LR : 1.8209219858156032e-05, Avg Loss : 0.4202\n","Epoch: 0, Step : 110, LR : 1.8031914893617023e-05, Avg Loss : 0.4146\n","Epoch: 0, Step : 120, LR : 1.7854609929078013e-05, Avg Loss : 0.4029\n","Epoch: 0, Step : 130, LR : 1.7677304964539008e-05, Avg Loss : 0.4172\n","Epoch: 0, Step : 140, LR : 1.7500000000000002e-05, Avg Loss : 0.3843\n","Epoch: 0, Step : 150, LR : 1.7322695035460996e-05, Avg Loss : 0.4183\n","Epoch: 0, Step : 160, LR : 1.7145390070921987e-05, Avg Loss : 0.4128\n","Epoch: 0, Step : 170, LR : 1.696808510638298e-05, Avg Loss : 0.3950\n","Epoch: 0, Step : 180, LR : 1.6790780141843972e-05, Avg Loss : 0.4129\n","Epoch: 0, Step : 190, LR : 1.6613475177304966e-05, Avg Loss : 0.3602\n","Epoch: 0, Step : 200, LR : 1.6436170212765957e-05, Avg Loss : 0.4299\n","Epoch: 0, Step : 210, LR : 1.625886524822695e-05, Avg Loss : 0.3442\n","Epoch: 0, Step : 220, LR : 1.6081560283687945e-05, Avg Loss : 0.3948\n","Epoch: 0, Step : 230, LR : 1.590425531914894e-05, Avg Loss : 0.3860\n","Epoch: 0, Step : 240, LR : 1.572695035460993e-05, Avg Loss : 0.3534\n","Epoch: 0, Step : 250, LR : 1.5549645390070924e-05, Avg Loss : 0.3574\n","Epoch: 0, Step : 260, LR : 1.5372340425531915e-05, Avg Loss : 0.3591\n","Epoch: 0, Step : 270, LR : 1.5195035460992908e-05, Avg Loss : 0.3997\n","Epoch: 0, Step : 280, LR : 1.5017730496453902e-05, Avg Loss : 0.3699\n","Epoch 0 Total Mean Loss : 0.4251\n","*****Epoch 0 Train Finish*****\n","\n","*****Epoch 0 Valid Start*****\n","Epoch 0 Valid Loss : 0.2407 Valid Acc : 92.19\n","*****Epoch 0 Valid Finish*****\n","\n","Saving epoch 0 checkpoint at ./model.ckpt.0\n","*****Epoch 1 Train Start*****\n","Epoch: 1, Step : 10, LR : 1.4804964539007095e-05, Avg Loss : 0.2493\n","Epoch: 1, Step : 20, LR : 1.4627659574468087e-05, Avg Loss : 0.2636\n","Epoch: 1, Step : 30, LR : 1.4450354609929078e-05, Avg Loss : 0.2908\n","Epoch: 1, Step : 40, LR : 1.427304964539007e-05, Avg Loss : 0.2928\n","Epoch: 1, Step : 50, LR : 1.4095744680851065e-05, Avg Loss : 0.2650\n","Epoch: 1, Step : 60, LR : 1.3918439716312057e-05, Avg Loss : 0.2811\n","Epoch: 1, Step : 70, LR : 1.3741134751773051e-05, Avg Loss : 0.3127\n","Epoch: 1, Step : 80, LR : 1.3563829787234044e-05, Avg Loss : 0.2867\n","Epoch: 1, Step : 90, LR : 1.3386524822695038e-05, Avg Loss : 0.3208\n","Epoch: 1, Step : 100, LR : 1.320921985815603e-05, Avg Loss : 0.2856\n","Epoch: 1, Step : 110, LR : 1.3031914893617021e-05, Avg Loss : 0.2804\n","Epoch: 1, Step : 120, LR : 1.2854609929078014e-05, Avg Loss : 0.2824\n","Epoch: 1, Step : 130, LR : 1.2677304964539008e-05, Avg Loss : 0.3198\n","Epoch: 1, Step : 140, LR : 1.25e-05, Avg Loss : 0.2334\n","Epoch: 1, Step : 150, LR : 1.2322695035460995e-05, Avg Loss : 0.2054\n","Epoch: 1, Step : 160, LR : 1.2145390070921987e-05, Avg Loss : 0.2538\n","Epoch: 1, Step : 170, LR : 1.196808510638298e-05, Avg Loss : 0.2834\n","Epoch: 1, Step : 180, LR : 1.1790780141843972e-05, Avg Loss : 0.3186\n","Epoch: 1, Step : 190, LR : 1.1613475177304965e-05, Avg Loss : 0.2660\n","Epoch: 1, Step : 200, LR : 1.1436170212765957e-05, Avg Loss : 0.2316\n","Epoch: 1, Step : 210, LR : 1.1258865248226952e-05, Avg Loss : 0.2968\n","Epoch: 1, Step : 220, LR : 1.1081560283687944e-05, Avg Loss : 0.2832\n","Epoch: 1, Step : 230, LR : 1.0904255319148938e-05, Avg Loss : 0.2881\n","Epoch: 1, Step : 240, LR : 1.072695035460993e-05, Avg Loss : 0.2692\n","Epoch: 1, Step : 250, LR : 1.0549645390070923e-05, Avg Loss : 0.3092\n","Epoch: 1, Step : 260, LR : 1.0372340425531916e-05, Avg Loss : 0.2500\n","Epoch: 1, Step : 270, LR : 1.0195035460992908e-05, Avg Loss : 0.2813\n","Epoch: 1, Step : 280, LR : 1.00177304964539e-05, Avg Loss : 0.2804\n","Epoch 1 Total Mean Loss : 0.2773\n","*****Epoch 1 Train Finish*****\n","\n","*****Epoch 1 Valid Start*****\n","Epoch 1 Valid Loss : 0.2803 Valid Acc : 85.94\n","*****Epoch 1 Valid Finish*****\n","\n","Saving epoch 1 checkpoint at ./model.ckpt.1\n","*****Epoch 2 Train Start*****\n","Epoch: 2, Step : 10, LR : 9.804964539007093e-06, Avg Loss : 0.2850\n","Epoch: 2, Step : 20, LR : 9.627659574468086e-06, Avg Loss : 0.2081\n","Epoch: 2, Step : 30, LR : 9.450354609929078e-06, Avg Loss : 0.2182\n","Epoch: 2, Step : 40, LR : 9.273049645390073e-06, Avg Loss : 0.1831\n","Epoch: 2, Step : 50, LR : 9.095744680851063e-06, Avg Loss : 0.1925\n","Epoch: 2, Step : 60, LR : 8.918439716312058e-06, Avg Loss : 0.1938\n","Epoch: 2, Step : 70, LR : 8.74113475177305e-06, Avg Loss : 0.1592\n","Epoch: 2, Step : 80, LR : 8.563829787234044e-06, Avg Loss : 0.1800\n","Epoch: 2, Step : 90, LR : 8.386524822695035e-06, Avg Loss : 0.1744\n","Epoch: 2, Step : 100, LR : 8.20921985815603e-06, Avg Loss : 0.1849\n","Epoch: 2, Step : 110, LR : 8.031914893617022e-06, Avg Loss : 0.2030\n","Epoch: 2, Step : 120, LR : 7.854609929078016e-06, Avg Loss : 0.1518\n","Epoch: 2, Step : 130, LR : 7.677304964539007e-06, Avg Loss : 0.1708\n","Epoch: 2, Step : 140, LR : 7.500000000000001e-06, Avg Loss : 0.2108\n","Epoch: 2, Step : 150, LR : 7.3226950354609935e-06, Avg Loss : 0.2127\n","Epoch: 2, Step : 160, LR : 7.145390070921986e-06, Avg Loss : 0.2329\n","Epoch: 2, Step : 170, LR : 6.968085106382979e-06, Avg Loss : 0.2286\n","Epoch: 2, Step : 180, LR : 6.790780141843972e-06, Avg Loss : 0.1752\n","Epoch: 2, Step : 190, LR : 6.613475177304965e-06, Avg Loss : 0.1619\n","Epoch: 2, Step : 200, LR : 6.436170212765958e-06, Avg Loss : 0.1591\n","Epoch: 2, Step : 210, LR : 6.258865248226951e-06, Avg Loss : 0.1962\n","Epoch: 2, Step : 220, LR : 6.081560283687944e-06, Avg Loss : 0.2082\n","Epoch: 2, Step : 230, LR : 5.904255319148937e-06, Avg Loss : 0.2564\n","Epoch: 2, Step : 240, LR : 5.7269503546099295e-06, Avg Loss : 0.1401\n","Epoch: 2, Step : 250, LR : 5.549645390070923e-06, Avg Loss : 0.1449\n","Epoch: 2, Step : 260, LR : 5.372340425531915e-06, Avg Loss : 0.1368\n","Epoch: 2, Step : 270, LR : 5.195035460992908e-06, Avg Loss : 0.2772\n","Epoch: 2, Step : 280, LR : 5.017730496453901e-06, Avg Loss : 0.1823\n","Epoch 2 Total Mean Loss : 0.1937\n","*****Epoch 2 Train Finish*****\n","\n","*****Epoch 2 Valid Start*****\n","Epoch 2 Valid Loss : 0.2668 Valid Acc : 90.62\n","*****Epoch 2 Valid Finish*****\n","\n","Saving epoch 2 checkpoint at ./model.ckpt.2\n","*****Epoch 3 Train Start*****\n","Epoch: 3, Step : 10, LR : 4.804964539007093e-06, Avg Loss : 0.1558\n","Epoch: 3, Step : 20, LR : 4.6276595744680855e-06, Avg Loss : 0.0975\n","Epoch: 3, Step : 30, LR : 4.450354609929078e-06, Avg Loss : 0.1879\n","Epoch: 3, Step : 40, LR : 4.273049645390071e-06, Avg Loss : 0.1768\n","Epoch: 3, Step : 50, LR : 4.095744680851064e-06, Avg Loss : 0.1354\n","Epoch: 3, Step : 60, LR : 3.918439716312057e-06, Avg Loss : 0.0952\n","Epoch: 3, Step : 70, LR : 3.74113475177305e-06, Avg Loss : 0.1195\n","Epoch: 3, Step : 80, LR : 3.5638297872340426e-06, Avg Loss : 0.1468\n","Epoch: 3, Step : 90, LR : 3.386524822695036e-06, Avg Loss : 0.1529\n","Epoch: 3, Step : 100, LR : 3.2092198581560285e-06, Avg Loss : 0.1218\n","Epoch: 3, Step : 110, LR : 3.031914893617022e-06, Avg Loss : 0.1654\n","Epoch: 3, Step : 120, LR : 2.8546099290780144e-06, Avg Loss : 0.1204\n","Epoch: 3, Step : 130, LR : 2.6773049645390077e-06, Avg Loss : 0.1270\n","Epoch: 3, Step : 140, LR : 2.5e-06, Avg Loss : 0.2464\n","Epoch: 3, Step : 150, LR : 2.322695035460993e-06, Avg Loss : 0.1074\n","Epoch: 3, Step : 160, LR : 2.145390070921986e-06, Avg Loss : 0.1515\n","Epoch: 3, Step : 170, LR : 1.968085106382979e-06, Avg Loss : 0.1403\n","Epoch: 3, Step : 180, LR : 1.790780141843972e-06, Avg Loss : 0.1489\n","Epoch: 3, Step : 190, LR : 1.6134751773049648e-06, Avg Loss : 0.1938\n","Epoch: 3, Step : 200, LR : 1.4361702127659578e-06, Avg Loss : 0.1341\n","Epoch: 3, Step : 210, LR : 1.2588652482269503e-06, Avg Loss : 0.1089\n","Epoch: 3, Step : 220, LR : 1.0815602836879434e-06, Avg Loss : 0.1155\n","Epoch: 3, Step : 230, LR : 9.042553191489363e-07, Avg Loss : 0.1119\n","Epoch: 3, Step : 240, LR : 7.26950354609929e-07, Avg Loss : 0.1794\n","Epoch: 3, Step : 250, LR : 5.496453900709221e-07, Avg Loss : 0.1637\n","Epoch: 3, Step : 260, LR : 3.723404255319149e-07, Avg Loss : 0.1398\n","Epoch: 3, Step : 270, LR : 1.9503546099290782e-07, Avg Loss : 0.2201\n","Epoch: 3, Step : 280, LR : 1.773049645390071e-08, Avg Loss : 0.1327\n","Epoch 3 Total Mean Loss : 0.1459\n","*****Epoch 3 Train Finish*****\n","\n","*****Epoch 3 Valid Start*****\n","Epoch 3 Valid Loss : 0.3266 Valid Acc : 90.62\n","*****Epoch 3 Valid Finish*****\n","\n","Saving epoch 3 checkpoint at ./model.ckpt.3\n","Train Completed. End Program.\n"]}],"source":["# 4 epoch 학습\n","epochs=4\n","model, optimizer, scheduler = initializer(train_dataloader, epochs)\n","train(model, train_dataloader, valid_dataloader, epochs)"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2022-02-02T03:27:18.246441Z","start_time":"2022-02-02T03:27:18.236617Z"},"id":"vA3_vqqCXccc"},"source":["### 가장 dev acc 성능이 높았던 epoch의 모델의 체크 포인트를 불러와 로드하자"]},{"cell_type":"code","execution_count":27,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:27.646150Z","start_time":"2022-02-02T06:22:26.945572Z"},"id":"mvfkSff25Q3z","executionInfo":{"status":"ok","timestamp":1646292868352,"user_tz":-540,"elapsed":6412,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# torch.load 함수 사용\n","\n","checkpoint = torch.load('./model.ckpt.0')"]},{"cell_type":"code","execution_count":28,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:36.415665Z","start_time":"2022-02-02T06:22:36.407250Z"},"id":"YqcxMmTj5Q3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292868353,"user_tz":-540,"elapsed":12,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"b7e14f6c-42e3-4cda-ba5e-795e93236e1f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'loss'])"]},"metadata":{},"execution_count":28}],"source":["# checkpoint의 key 종류를 확인\n","checkpoint.keys()"]},{"cell_type":"code","execution_count":29,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:40.272939Z","start_time":"2022-02-02T06:22:37.010491Z"},"id":"wTvFYgNi5Q30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292870824,"user_tz":-540,"elapsed":2476,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"7fe76132-7fc3-4719-a532-fce0998f05cd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Total train steps with 1 epochs: 282\n"]}],"source":["# 위에서 구현한 initializer 함수 사용하여 model, optimizer, scheduler 초기화\n","\n","epochs=1\n","model, optimizer, scheduler = initializer(train_dataloader, epochs)"]},{"cell_type":"code","execution_count":30,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:40.443912Z","start_time":"2022-02-02T06:22:40.274323Z"},"id":"CtR2sTW55Q30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292870828,"user_tz":-540,"elapsed":14,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"84db68ef-e35c-4899-930f-ecfd47368874"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":30}],"source":["model.load_state_dict(checkpoint[\"model_state_dict\"])"]},{"cell_type":"markdown","metadata":{"id":"Tzske7SR5Q30"},"source":["### 모델 예측 함수 구현\n","- test_dataloader를 입력받아 모델이 예측한 확률값 (probs)과 실제 정답 (label) 을 출력하는 `\bpredict()` 함수를 구현하자.\n","- 함수 정의\n","  - 입력 매개변수\n","    - `model` : `CustomClassifier` 모델. logits를 반환함 \n","    - `test_dataloader` : test 데이터셋의 텍스트와 레이블을 배치로 갖는 dataloader\n","  - 조건\n","    - `test_dataloader`는 이터레이터기 때문에 이터레이터를 순회하면서 `all_logits` 리스트에 배치 단위의 logits를 저장하고 `all_labels` 리스트에 배치 단위의 레이블 (0 또는 1 값)을 저장하라\n","  - 반환값\n","    - `probs`\n","      - logits에 softmax 함수를 취한 확률값. (test data 개수, label 개수) shape을 가짐. \bnp.array 타입으로 데이터 타입을 변환할 것.\n","    - `labels`\n","      - 0 또는 1 값을 갖는 np.array. (test data 개수,) shape을 가짐."]},{"cell_type":"code","execution_count":31,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:48.062229Z","start_time":"2022-02-02T06:22:48.057531Z"},"id":"yQ7WiD1Oigg9","executionInfo":{"status":"ok","timestamp":1646292870831,"user_tz":-540,"elapsed":12,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["def predict(model, test_dataloader):\n","    \"\"\"\n","    test_dataloader의 label별 확률값과 실제 label 값을 반환\n","    \"\"\"\n","\n","    # model을 eval 모드로 설정 & device 할당\n","    model.eval()\n","    model.to(device)\n","\n","    all_logits = []\n","    all_labels = []\n","\n","    for step, batch in enumerate(test_dataloader):\n","        \n","        batch_input, batch_label = batch\n","        \n","        # batch_input을 device 할당\n","        batch_input.to(device)\n","\n","        # model에 batch_input을 넣어 logit 반환 & all_logits, all_labels 리스트에 값 추가 \n","        with torch.no_grad():\n","            logits = model(**batch_input)\n","        all_logits.append(logits)\n","        all_labels.append(batch_label)\n","    \n","\n","    probs = [F.softmax(i, dim=1).cpu().numpy() for i in all_logits]\n","    probs = np.array([i for j in probs for i in j]) # logits을 확률값으로 변환 & Tensor 타입을 numpy.array 타입으로 변환\n","    \n","    all_labels = [i.numpy() for i in all_labels]\n","    all_labels = np.array([i for j in all_labels for i in j]) #  Tensor 타입을 numpy.array 타입으로 변환\n","\n","    return probs, all_labels\n","\n"]},{"cell_type":"markdown","source":["- 모델이 예측한 확률값과 실제 label을 입력 받아 정확도를 출력하는 **accuracy()** 함수를 구현하자. \n","- 함수 정의 \n","  - 입력 매개변수 \n","    - `probs` : `predict()` 함수의 반환값. 2차원의 np.array\n","    - `labels` : `predict()` 함수의 반환값. 1차원의 np.array\n","  - 조건\n","    - `probs`의 확률값이 0.5 이상이면 1, 이하이면 0이 되도록 만든다. 모델이 예측한 레이블을 실제값(`labels`)과 비교해 예측값과 실제값이 같으면 1, 다르면 0 점수를 준다. 모든 데이터에 대해 점수의 평균값이 accuracy 값이다. \n","  - 반환값 \n","    - `acc` : 정확도 (Float type)"],"metadata":{"id":"lOxCjZ2g6ZeK"}},{"cell_type":"code","execution_count":32,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:48.296419Z","start_time":"2022-02-02T06:22:48.293737Z"},"id":"42-umZ3m5Q32","executionInfo":{"status":"ok","timestamp":1646292870832,"user_tz":-540,"elapsed":13,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["# accuracy 함수 구현\n","def accuracy(probs, labels):\n","    y_pred = np.argmax(probs, axis=1)\n","    # np.where(probs>=0.5, 1, 0) # probs(확률값)을 label로 변경(0.5 이상이면 1, 0.5 미만이면 0)\n","    acc = np.where(labels==y_pred, 1, 0).mean() # 정확도 계산\n","    return acc"]},{"cell_type":"code","execution_count":33,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:22.752497Z","start_time":"2022-02-02T06:22:48.652784Z"},"id":"SwkrRPAhjsXb","executionInfo":{"status":"ok","timestamp":1646292875860,"user_tz":-540,"elapsed":5041,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["probs, labels = predict(model, test_dataloader)"]},{"cell_type":"code","execution_count":34,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:22.759367Z","start_time":"2022-02-02T06:24:22.753997Z"},"id":"MxDI8PRA5Q32","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292875860,"user_tz":-540,"elapsed":18,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"f4be1a00-dc8a-4ffc-fd6e-da82bbaedebe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.868"]},"metadata":{},"execution_count":34}],"source":["accuracy(probs, labels)"]},{"cell_type":"markdown","metadata":{"id":"3mqUfkx-5Q33"},"source":["### `sklearn.metrics`의 `accuracy_score`, `roc_auc_score` 함수를 이용해 정확도와 auc를 계산하라"]},{"cell_type":"code","execution_count":35,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:23.111879Z","start_time":"2022-02-02T06:24:22.760568Z"},"id":"VFWj4lcp5Q33","executionInfo":{"status":"ok","timestamp":1646292875861,"user_tz":-540,"elapsed":17,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}}},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, accuracy_score"]},{"cell_type":"code","execution_count":36,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:23.116872Z","start_time":"2022-02-02T06:24:23.113064Z"},"id":"p9BEe2mflTem","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292875861,"user_tz":-540,"elapsed":17,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"669749bb-21ab-4012-c35a-6d33cd1b1d08"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.868"]},"metadata":{},"execution_count":36}],"source":["# 정확도 출력\n","\n","accuracy_score(np.argmax(probs, axis=1), labels)"]},{"cell_type":"code","execution_count":37,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:23.125650Z","start_time":"2022-02-02T06:24:23.117847Z"},"id":"oCl6BiPGpCPW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646292875862,"user_tz":-540,"elapsed":10,"user":{"displayName":"김성우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09699747628091584373"}},"outputId":"0c0c3623-017f-4ed0-8830-b93c1d26f4af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8688498300083392"]},"metadata":{},"execution_count":37}],"source":["# auc 출력\n","\n","roc_auc_score(np.argmax(probs, axis=1), labels)"]},{"cell_type":"code","source":[""],"metadata":{"id":"01lmOHdms5-h"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"김성우 - Week2_4_assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}